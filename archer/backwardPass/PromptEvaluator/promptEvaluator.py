# Evaluate which prompts beat the existing prompts from the previous round. We want to keep a specified quantile as defined in the object of the prompt evaluator object, what quantile we want to keep from all of them. So based on the, so from the existing, we're going to have two inputs, we're going to have like the existing prompts and the new prompts which have to be evaluated. We will create ai automatically, we will ai score using the evaluator tool to automatically score the new evaluations using just ai and then we will keep the top specified quantile of prompts based on the score generated by ai or the human.

from typing import List, Tuple, Any, Dict, Union, Optional
from helpers.prompt import Prompt
from forwardPass.evaluator import AIExpert
from forwardPass.generator import GenerativeModel
from ...helpers.logging_utils import get_logger, log_entry_exit, log_call_args

# Setup logger for this module
logger = get_logger(__name__)

class PromptEvaluator:
    """
    Simulates an AI-driven forward pass evaluation for prompt variants.
    
    This class evaluates prompt variants by:
      1. Generating content using the provided generative model with a given input.
      2. Evaluating the generated content using a frozen AI expert evaluator.
      3. Aggregating the results over multiple simulation runs to compute an average score.
    
    The output is used in the backward pass to select and evolve the best prompts.
    """

    def __init__(self, generative_model: GenerativeModel, evaluator: AIExpert, num_simulations: int = 3,
                 quantile_threshold: float = 0.25):
        """
        Initialize a new PromptEvaluator.
        
        Args:
            generative_model: An instance of GenerativeModel to generate content.
            evaluator: An instance of AIExpert to evaluate generated content.
            num_simulations: Number of simulation runs per prompt (default: 3).
            quantile_threshold: Fraction of top-performing prompts to select (default: 0.25).
        """
        logger.info("Initializing PromptEvaluator")
        self.generative_model = generative_model
        self.evaluator = evaluator
        self.num_simulations = num_simulations
        self.quantile_threshold = quantile_threshold
        logger.info(f"PromptEvaluator initialized with {num_simulations} simulations and {quantile_threshold} quantile threshold")

    @log_entry_exit(logger)
    def evaluate_prompts(self, prompts: List[Prompt], input_data: Any, 
                        num_simulations: Optional[int] = None) -> List[Tuple[Prompt, float, List[Dict[str, Any]]]]:
        """
        Evaluate a list of prompts by simulating multiple forward pass generations and evaluations.
        
        For each prompt, the process is:
            1. Temporarily use the prompt as the active prompt in the generative model.
            2. Generate content using the given input_data.
            3. Evaluate the generated content with the AI expert evaluator.
            4. Aggregate the scores over several simulation runs.
        
        Args:
            prompts: List of Prompt objects to evaluate.
            input_data: Input data to pass to the generative model. Can be a single item or a list of items.
            num_simulations: Optional override for the number of simulations to run per prompt.
                            If not provided, uses the instance's num_simulations value.
        
        Returns:
            A list of tuples for each prompt containing:
                - The prompt object.
                - The average score (float) from the simulations.
                - A list of detailed evaluation result dictionaries from each simulation run.
        """
        logger.info(f"Evaluating {len(prompts)} prompts")
        
        results = []
        simulations = num_simulations if num_simulations is not None else self.num_simulations
        logger.info(f"Running {simulations} simulations per prompt")
        
        # Handle input_data as either a single item or a list of items
        inputs = input_data if isinstance(input_data, list) else [input_data]
        logger.debug(f"Using {len(inputs)} input items for evaluation")
        
        for i, prompt in enumerate(prompts):
            logger.info(f"Evaluating prompt {i+1}/{len(prompts)}: {prompt.content[:50]}...")
            
            simulation_results = []
            scores = []
            # Set the active prompt to just this one to isolate its simulation.
            logger.debug(f"Setting generator to use only the current prompt")
            self.generative_model.set_prompts([prompt])
            
            # Distribute the simulations across the available inputs
            for j in range(simulations):
                # Cycle through the inputs if we have more simulations than inputs
                current_input = inputs[j % len(inputs)]
                logger.debug(f"Running simulation {j+1}/{simulations} with input #{j % len(inputs) + 1}")
                
                # Generate content; since only one prompt is active, we expect one output.
                outputs = self.generative_model.generate(current_input)
                if outputs:
                    generated_content, used_prompt = outputs[0]
                    logger.debug(f"Generated content length: {len(generated_content)} chars")
                    
                    # Evaluate the generated content using the frozen AI expert evaluator.
                    logger.debug(f"Evaluating generated content")
                    evaluation = self.evaluator.evaluate(generated_content, current_input)
                    score = evaluation.get('score', 0)
                    logger.debug(f"Evaluation score: {score}")
                    
                    scores.append(score)
                    simulation_results.append(evaluation)
                else:
                    logger.warning(f"No output generated for prompt in simulation {j+1}")
            
            # Compute the average score for the prompt.
            if scores:
                avg_score = sum(scores) / len(scores)
                logger.info(f"Prompt average score: {avg_score} from {len(scores)} simulations")
            else:
                avg_score = 0
                logger.warning(f"No scores collected for prompt, using default score of 0")
                
            results.append((prompt, avg_score, simulation_results))
        
        logger.info(f"Prompt evaluation complete, evaluated {len(results)} prompts")
        return results
    
    @log_entry_exit(logger)
    def select_best_prompts(self, prompt_results: List[Tuple[Prompt, float, List[Dict[str, Any]]]],
                           quantile: Optional[float] = None) -> List[Tuple[Prompt, float, List[Dict[str, Any]]]]:
        """
        Select the best performing prompts based on a quantile threshold.
        
        Args:
            prompt_results: List of (prompt, score, details) tuples from evaluate_prompts.
            quantile: Fraction of prompts to keep (e.g., 0.25 means top 25%).
                     If None, uses the instance's quantile_threshold.
                    
        Returns:
            List of (prompt, score, details) tuples for the best prompts.
        """
        logger.info(f"Selecting best prompts from {len(prompt_results)} candidates")
        
        if not prompt_results:
            logger.warning("No prompt results to select from, returning empty list")
            return []
            
        threshold = quantile if quantile is not None else self.quantile_threshold
        logger.info(f"Using quantile threshold: {threshold}")
        
        # Sort by score in descending order
        sorted_results = sorted(prompt_results, key=lambda x: x[1], reverse=True)
        
        # Calculate how many prompts to keep, at least 1
        keep_count = max(1, int(len(sorted_results) * threshold))
        logger.info(f"Keeping top {keep_count} prompts")
        
        # Return the top performers
        best_prompts = sorted_results[:keep_count]
        
        # Log the scores of selected prompts
        for i, (prompt, score, _) in enumerate(best_prompts):
            logger.debug(f"Selected prompt #{i+1} with score {score}: {prompt.content[:50]}...")
            
        return best_prompts
    
    @log_entry_exit(logger)
    def evaluate_and_select_best(self, prompts: List[Prompt], input_data: Any,
                               quantile: Optional[float] = None,
                               num_simulations: Optional[int] = None) -> List[Prompt]:
        """
        Evaluate prompts and select the best performing ones in a single operation.
        
        Args:
            prompts: List of Prompt objects to evaluate.
            input_data: Input data to pass to the generative model.
            quantile: Fraction of prompts to keep (e.g., 0.25 means top 25%).
            num_simulations: Number of simulations to run per prompt.
            
        Returns:
            List of the best performing Prompt objects.
        """
        logger.info(f"Evaluating and selecting best prompts from {len(prompts)} candidates")
        
        # Evaluate all prompts
        logger.info("Starting prompt evaluation")
        evaluation_results = self.evaluate_prompts(
            prompts=prompts,
            input_data=input_data,
            num_simulations=num_simulations
        )
        
        # Select the best ones
        logger.info("Selecting best prompts from evaluation results")
        best_prompts = self.select_best_prompts(
            prompt_results=evaluation_results,
            quantile=quantile
        )
        
        # Return just the Prompt objects (first element of each tuple)
        result_prompts = [result[0] for result in best_prompts]
        logger.info(f"Returning {len(result_prompts)} best prompts")
        return result_prompts
