# Evaluate which prompts beat the existing prompts from the previous round. We want to keep a specified quantile as defined in the object of the prompt evaluator object, what quantile we want to keep from all of them. So based on the, so from the existing, we're going to have two inputs, we're going to have like the existing prompts and the new prompts which have to be evaluated. We will create ai automatically, we will ai score using the evaluator tool to automatically score the new evaluations using just ai and then we will keep the top specified quantile of prompts based on the score generated by ai or the human.

from typing import List, Tuple, Any, Dict, Union, Optional
from helpers.prompt import Prompt
from forwardPass.evaluator import AIExpert
from forwardPass.generator import GenerativeModel

class PromptEvaluator:
    """
    Simulates an AI-driven forward pass evaluation for prompt variants.
    
    This class evaluates prompt variants by:
      1. Generating content using the provided generative model with a given input.
      2. Evaluating the generated content using a frozen AI expert evaluator.
      3. Aggregating the results over multiple simulation runs to compute an average score.
    
    The output is used in the backward pass to select and evolve the best prompts.
    """

    def __init__(self, generative_model: GenerativeModel, evaluator: AIExpert, num_simulations: int = 3,
                 quantile_threshold: float = 0.25):
        """
        Initialize a new PromptEvaluator.
        
        Args:
            generative_model: An instance of GenerativeModel to generate content.
            evaluator: An instance of AIExpert to evaluate generated content.
            num_simulations: Number of simulation runs per prompt (default: 3).
            quantile_threshold: Fraction of top-performing prompts to select (default: 0.25).
        """
        self.generative_model = generative_model
        self.evaluator = evaluator
        self.num_simulations = num_simulations
        self.quantile_threshold = quantile_threshold

    def evaluate_prompts(self, prompts: List[Prompt], input_data: Any, 
                        num_simulations: Optional[int] = None) -> List[Tuple[Prompt, float, List[Dict[str, Any]]]]:
        """
        Evaluate a list of prompts by simulating multiple forward pass generations and evaluations.
        
        For each prompt, the process is:
            1. Temporarily use the prompt as the active prompt in the generative model.
            2. Generate content using the given input_data.
            3. Evaluate the generated content with the AI expert evaluator.
            4. Aggregate the scores over several simulation runs.
        
        Args:
            prompts: List of Prompt objects to evaluate.
            input_data: Input data to pass to the generative model. Can be a single item or a list of items.
            num_simulations: Optional override for the number of simulations to run per prompt.
                            If not provided, uses the instance's num_simulations value.
        
        Returns:
            A list of tuples for each prompt containing:
                - The prompt object.
                - The average score (float) from the simulations.
                - A list of detailed evaluation result dictionaries from each simulation run.
        """
        results = []
        simulations = num_simulations if num_simulations is not None else self.num_simulations
        
        # Handle input_data as either a single item or a list of items
        inputs = input_data if isinstance(input_data, list) else [input_data]
        
        for prompt in prompts:
            simulation_results = []
            scores = []
            # Set the active prompt to just this one to isolate its simulation.
            self.generative_model.set_prompts([prompt])
            
            # Distribute the simulations across the available inputs
            for i in range(simulations):
                # Cycle through the inputs if we have more simulations than inputs
                current_input = inputs[i % len(inputs)]
                
                # Generate content; since only one prompt is active, we expect one output.
                outputs = self.generative_model.generate(current_input)
                if outputs:
                    generated_content, used_prompt = outputs[0]
                    # Evaluate the generated content using the frozen AI expert evaluator.
                    evaluation = self.evaluator.evaluate(generated_content, current_input)
                    score = evaluation.get('score', 0)
                    scores.append(score)
                    simulation_results.append(evaluation)
            
            # Compute the average score for the prompt.
            avg_score = sum(scores) / len(scores) if scores else 0
            results.append((prompt, avg_score, simulation_results))
        
        return results
    
    def select_best_prompts(self, prompt_results: List[Tuple[Prompt, float, List[Dict[str, Any]]]],
                           quantile: Optional[float] = None) -> List[Tuple[Prompt, float, List[Dict[str, Any]]]]:
        """
        Select the best performing prompts based on a quantile threshold.
        
        Args:
            prompt_results: List of (prompt, score, details) tuples from evaluate_prompts.
            quantile: Fraction of prompts to keep (e.g., 0.25 means top 25%).
                     If None, uses the instance's quantile_threshold.
                    
        Returns:
            List of (prompt, score, details) tuples for the best prompts.
        """
        if not prompt_results:
            return []
            
        threshold = quantile if quantile is not None else self.quantile_threshold
        
        # Sort by score in descending order
        sorted_results = sorted(prompt_results, key=lambda x: x[1], reverse=True)
        
        # Calculate how many prompts to keep, at least 1
        keep_count = max(1, int(len(sorted_results) * threshold))
        
        # Return the top performers
        return sorted_results[:keep_count]
    
    def evaluate_and_select_best(self, prompts: List[Prompt], input_data: Any,
                               quantile: Optional[float] = None,
                               num_simulations: Optional[int] = None) -> List[Prompt]:
        """
        Evaluate prompts and select the best performing ones in a single operation.
        
        Args:
            prompts: List of Prompt objects to evaluate.
            input_data: Input data to pass to the generative model.
            quantile: Fraction of prompts to keep (e.g., 0.25 means top 25%).
            num_simulations: Number of simulations to run per prompt.
            
        Returns:
            List of the best performing Prompt objects.
        """
        # Evaluate all prompts
        evaluation_results = self.evaluate_prompts(
            prompts=prompts,
            input_data=input_data,
            num_simulations=num_simulations
        )
        
        # Select the best ones
        best_prompts = self.select_best_prompts(
            prompt_results=evaluation_results,
            quantile=quantile
        )
        
        # Return just the Prompt objects (first element of each tuple)
        return [result[0] for result in best_prompts]
